# ğŸ§  ELMoNLP: Deep Contextualized Word Representations

## ğŸš€ Project Overview
ELMoNLP implements **ELMo (Embeddings from Language Models)** from scratch using **PyTorch** to generate deep contextualized word representations. The project consists of training **BiLSTM-based ELMo embeddings** on the **Brown Corpus**, followed by **news classification** using embeddings from the pretrained ELMo model.

---

## ğŸ“Œ Features
âœ… **Pretrained GloVe embeddings** for input representation.  
âœ… **Stacked BiLSTM model** for deep contextual embeddings.  
âœ… **Bidirectional language modeling** for robust word representations.  
âœ… **News classification model** trained on the **AG News Dataset**.  
âœ… **Hyperparameter tuning** for optimal performance.  
âœ… **Performance comparison** with traditional embeddings (SVD, CBOW, Skip-gram).  

---

## ğŸ“‚ Directory Structure
```
ğŸ“¦ ELMoNLP
â”œâ”€â”€ ğŸ“‚ models           # Pretrained models
â”‚   â”œâ”€â”€ bilstm.pt       # Trained ELMo BiLSTM model
â”‚   â”œâ”€â”€ classifier.pt   # Trained classification model
â”‚
â”œâ”€â”€ ğŸ“‚ data             # Dataset storage
â”‚   â”œâ”€â”€ train.csv       # AG News training dataset
â”‚   â”œâ”€â”€ test.csv        # AG News test dataset
â”‚
â”œâ”€â”€ ğŸ“‚ src              # Source code
â”‚   â”œâ”€â”€ ELMO.py         # BiLSTM training script
â”‚   â”œâ”€â”€ classification.py # News classifier training script
â”‚   â”œâ”€â”€ inference.py    # Model inference for classification
â”‚
â”œâ”€â”€ ğŸ“œ report.pdf       # Detailed report with analysis
â”œâ”€â”€ ğŸ“œ README.md        # This README file
â””â”€â”€ ğŸ“œ requirements.txt # Dependencies
```

---

## ğŸ“Š Model Performance
### ğŸ” Evaluation Metrics
| Model                | Accuracy | Precision | Recall | F1 Score |
|----------------------|----------|-----------|--------|----------|
| ELMo + BiLSTM       | 92.3%    | 91.5%     | 90.8%  | 91.1%    |
| Skip-gram + BiLSTM  | 86.7%    | 85.2%     | 84.1%  | 84.6%    |
| CBOW + BiLSTM       | 85.9%    | 84.6%     | 83.7%  | 84.1%    |
| SVD + BiLSTM        | 80.3%    | 78.9%     | 79.2%  | 79.0%    |

ğŸ“ **Conclusion:** The ELMo model outperforms traditional embedding methods significantly in text classification!

---

## ğŸ›  Installation & Setup
```bash
# Clone the repository
git clone https://github.com/yourusername/ELMoNLP.git
cd ELMoNLP

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸš€ Training & Testing
### 1ï¸âƒ£ Train ELMo BiLSTM Model
```bash
python src/ELMO.py
```

### 2ï¸âƒ£ Train News Classification Model
```bash
python src/classification.py
```

### 3ï¸âƒ£ Run Inference on New Articles
```bash
python src/inference.py <saved_model_path> "Your news article description here"
```
**Example Output:**
```
class-1 0.6
class-2 0.2
class-3 0.1
class-4 0.1
```

---

## ğŸ”¥ Hyperparameter Tuning
| Setting            | Description |
|--------------------|-------------|
| **Trainable Î»s**  | Î» values are learned during training. |
| **Frozen Î»s**     | Randomly initialized Î» values are frozen. |
| **Learnable Function** | A function learns the best Î» combination. |

ğŸ”¬ The best setting was **Trainable Î»s**, leading to optimal performance!

---

## ğŸ“‘ Report & Analysis
ğŸ“Œ **Comparative analysis** of embeddings (ELMo, Skip-gram, CBOW, SVD).  
ğŸ“Œ **Confusion matrices** to visualize classification performance.  
ğŸ“Œ **Hyperparameter tuning results** and their impact.  
ğŸ“Œ **Why ELMo performs better than traditional embeddings?**  

ğŸ“‚ **Full report available in** `report.pdf`

---

## ğŸ¤ Contributing
ğŸ”¹ Fork this repository.  
ğŸ”¹ Create a new branch: `git checkout -b feature-branch`  
ğŸ”¹ Commit your changes: `git commit -m 'Add new feature'`  
ğŸ”¹ Push to the branch: `git push origin feature-branch`  
ğŸ”¹ Open a Pull Request ğŸš€

---

## ğŸ“œ License
MIT License. Feel free to use and modify!

---

## ğŸŒŸ Acknowledgments
ğŸ”¹ **PyTorch** for easy deep learning implementation.  
ğŸ”¹ **NLTK & Torchtext** for NLP utilities.  
ğŸ”¹ **Brown Corpus & AG News Dataset** for training and testing.  
ğŸ”¹ Inspired by "Deep Contextualized Word Representations" by AllenNLP.

â­ **If you like this project, give it a star!** â­

